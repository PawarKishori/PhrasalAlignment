Anusaaraka Task:

Goal of Anusaaraka is to create word aligned parallel corpora using following resourses:
1. Anusaaraka output
2. Statistical Machine Translation output.
3. Alignment of Parser output

-----------------------------------------------------------------------------------------------------
My Task:

My task focuses on 3rd point.
Which will lead to a good nth layer generation.
One of my goals is to automate the creation of Hindi Treebank using output of phrasal alignment tool.

When the automated and correct hindi treebank is being created it can be used in the following tasks:
1. Creation of Hindi Parser(either by graph based learning or something else)
2. Gold Treebank created from automation and some rules.
3. A system to create automated hindi treebank from english-hindi parallel corpora.

------------------------------------------------------------------------------------------------------
Algorithm for automation system:

1. Do projection of relations from english to hindi if and only if
   a. E-H translation is very close i.e. All words eng and hindi words are aligned properly. (Unaligned words ==0) (From the data statistically can I check this condition**)
   b. sentences without Conjunction relation (for time being)
   c. sentences without prepositional attachment (for time being)
   d. Parse given by stanford for english sentence is correct.
   
4. Rule Step and/or learning step
   Do analysis and find out the rule 
      if u can find out the rule then do 2.1:
	2.1.
	If the relation from UD-english are one-many write the rules forappropriate relation.
        (As per the discussion, we can write rules for following relations )
        a. acl
	b. appos
	c. nsubjpass
	   etc.
      else do 2.2:
	2.2.
        Collect the different (as many as u can) examples from the corpora and real time (from linguists), then put all the data in structured format the apply some learning technique (like GCNN or something*) and learn the rules from the data instead of learning the data (which many statistical and ml based techniques does)
2. One to one relation mapping from english to hindi
   a. nsubj
   b. nmod 
	etc.
   
3. If we have mapped relations(from mapped relation dictionary**) from one relation name(in english) to the relation name(in hindi) then map such relations.
   a. advmod (BI) -> dep
																																																		
5. Prepare dictionary for such alignments.
   That is why - isiliye

------------------------------------------------------------------------------------------------------------
Number of columns for observations of sentences

01. English sentence
02. Hindi Sentence
03. Sentence id in phrasal alignment tool
04. Algorithm Step 1 Satisfied? 
05. number of interchunk relations in english sentence (ignore intrachunk for timebeing)
06. number of interchunk relations in hindi sentence
07. number of exact mapped relations (algo step 4)
08. list of 7 (algo step 4)
09. number of relations (algo step 3)
10. list of relations (algo step 3)
11. number of relations (algo step 2)
12. list of relations (algo step 2)
13. Rules extracted for the relation (algo step 2.1)
14. Dictionary observations




** work on this point
-------------------------------------------------------------------------------------------------
Update: 2-Dec-2017 : Meeting with Chaitanya Sir and Soma mam.
My ultimate aim is to improve parsers output, both Hindi and English.
So I need to create a better hindi parser too.
How I can do that? - Using Alignment tool developed which uses bootstrapping.
Alignment tool needs output of hindi and english parser. English  parser performs better than hindi parser. So primarily I need to focus on hindi parser first.
Alignment tool needs hindi parser to parse hindi sentences. Till now I was using hindi rule based parser developed by Roja mam. It's latest version was giving multiple relations which I cant use in alignment tool, and it was not that efficient too. So we decided to use Irshad's parser (simple neural network - 90% accuracy claimed) https://bitbucket.org/iscnlp/ which was trained on Hindi Treebank.
hindi_irshad.txt is output of this NNparser ran on hindi_raw.txt
The parser gives output in UD dep relations as well as paninian dep relations.
Sir claim that Paninian dep relations of irshad and us differ at some places.
Todays task - checking the accuracy of irshad's hindi parser (manually, with sir's paninian concept) and checking whether we can use that hindi parser into alignment tool for further bootstrapping.
hindi_irshad.txt    => wrongly annotated dep relations
                    *  Discussion required

-------------------------------------------------------------------------------------------------
3-dec-2017
Gathering of interchunk and intrachunk paninian dependencies from Pruthwik, Dipti mam
------------------------------------------------------------------------------------------------- 
4-dec-2017 
Error checking of parser output according to guidelines provided.
-------------------------------------------------------------------------------------------------
6-dec-2017
code to convert parser output to convinient reading format
-------------------------------------------------------------------------------------------------7
7-dec-2017
Earlier I ran it on training data(might be not sure ), so need to run the parser on new test data. I am running it on Gyan-Nidhi_en_hi_anoop corpus (500 sentences)
Task: Divide output into set of 10 sentences, analyse 10 sentences each.
-------------------------------------------------------------------------------------------------7
8-dec-2017
The hindi parser output is not totally accurate but we can use it in Phrsal Alignment tool.
-------------------------------------------------------------------------------------------------7
9-dec-2017
Creation of (Dipti mam's) Paninian to (Anusaaraka's)Paninian mapping. And add one more layer to Phrasal alignment tool which will use Irshad's NN parser and show hindi parser output aligned with English.
---------------------------------------------------------------------------------------------------
11-dec-2017
 
 sh run.sh <phrase-table-en-hi> <phrase-table-hi-en> <corpus_name>
 sh run.sh phrases_en-hi  phrases_hi-en  physics
 sh run.sh gyan_nidhi_en_hi_wx gyan_nidhi_hi_en_wx gyan_nidhi

This command takes the phrase table of the corpus created by Phrasal tool and changes this phrase output into dictionary gdbm format.
The gdbm output(gyan_nidhi_en_hi_wx, gyan_nidhi_hi_en_wx) will be saved in /anusaaraka/miscellaneous/SMT/phrasal_alignment

To run Alignment tool on parallel corpus, input is parallel chunks of sentences given parallel corpus. These are obtained by training parallel corpus on a statistical tool - Phrsal tool. So on 30k gyan nidhi parallel sentences, Phrasal is ran and we got gyan_nidhi_en_hi_wx and gyan_nidhi_hi_en_wx. These are phrase tables we obtatined, which are stored in gyan_nighi folder 

sh run_alignment.sh e1 h1 gyan_nidhi
This command is used to run alignment tool on parallel corpus e1 and h1, which also uses gyan_nidhi dictionary (gdbm- created by above run.sh command)

After that we need to do compilation steps to run alignment data.

*****
Giving error

Debugging:
/anusaaraka$ git diff --namestatus
error: invalid option: --namestatus
master@kishori-ThinkCentre-E73:~/anusaaraka$ git diff --name-status
M       multifast-v1.4.2/src/phrasal_mwe/extract_hindi_key.c
M       multifast-v1.4.2/src/phrasal_mwe/extract_key-hi-en.c
M       multifast-v1.4.2/src/phrasal_mwe/extract_key.c

git checkout all these 3 files 
++ 
Did some changes in run_alignment.sh // which was not committed
******
After successfully running phrasal alignment output will be generated in following dir with filename_eng_slign.html file
firefox /home/master/anu_output/e1_eng_align.html 

The backend files of phrasal alignment will be in : 
cd /home/master/tmp_anu_dir/tmp/gyan_nidhi_e500_tmp/
-----------------------------------------------------------------------------------------------
12-dec
program to convert parser output into facts format(manju mam)
mapping of UD tagging, Enhanced UD tagging, Sukhada Paninian notation, Dipti Paninian Notation
---------------------------------------------------------------------------------------------
14- dec
Stanford dep(nsubj, nmod) -> Paninian dep by Sukhda(krIyA-to-saMbabXI,etc) => Done for old version of Stanford parser
Have to update stanford parser in anusaaraka, this will add/remove/change stanford dep to Enhanced stanford dep.
Then need to map extra relations into again Sukhada paninian dependency.
This will bring dependency representation of stanford to paninian dependency(Sukhada's) from english side.

Now I also need to map Dipti mam's paninian dependency(k1,k2,lwg_psp,etc) to Sukhada paninian dependency(kriyA-to-saMbanXI) which was given by hindi parser.

All these will lead to 
1. create a parse tree for english sentence by stanford parser, with sukhada's paninian relation.
2. create a parse tree for hindi sentence by irshad's neural network parser, with sukhada's paninian relation.
3. And Alignment tool will create a parse trees of english and hindi sentences with same paninian relation.
4. This will lead to development of  parallel treebank automatically using Alignment tool.
5. Note: 100% mapping of stanford dependency relations into paninian is not possible in one instance. So we need to do in as much as possible now. Then with bootstrapping this procedure will automatically map the relations with building corpus as much as we can.

============================
Observation and Analysis:
1goeswith : She came because she liked me.
stanford latest not giving the output.
2
The director is 65 years <--old : nmod:npmod
Enhanced dep giving ERROR: Failed to load collapsed_ccproc_dep data! Please contact the administrator.

3
I want students to go.
There was a kid playing in the garden.
I made him build the house.
I got the house built.

remaining observations and analysis is on drive
===========================================================================================
15-Jan-2018
Till now I got confused in various things, but now I have cleared following things and the obsrvations are as follows:

1. CoreNLP is whole pipeline where it does everything from POS, NER, Co-referencing, Parsing etc. Initially I was running whole CoreNLP, but I should have run only stanford parser.
2. After instalation of stanford parser I need to analyze the 2 dependency formats given by stanford parser
a) stanford dependencies - basic, collapsed, ccprocessed, collapsed tree, non-collapsed. 
b) universal dependencies - basic, enhanced, enhanced++

Bell, a company which is based in LA, makes and distributes computer products
1.basic
2.collapsed (grouping, graph)
3.ccprocessed (propogation => here we need to work more)
4. collapsed tree 
5. non-collapsed tree (basic+extra)

Similarly do observations for UD (basic, enhanced, enhanced++)
.....
.....

Create documentation of commands of all variation with options (-originalDependencies, -outputFormat, -outputFormatOptions), with default conditions and which command to use for which type of output.

U should know which output format from above outputs should be used to do necessary information extraction, which will be used by us in stnford-parser-english-anusaaraka-extension. U should be master in that.

The next task told by sir
Just like the analysis I am doing for stanford, I should also do it for following 3:
1. Parser which uses, MRS-DMRS structure
2. Stanford Parser
3. Syntaxnet used by DeepL (which is performing better than Microsoft and google (in Europian languages) due to attention-based model)

I need to do it from hindi parsing side too.

Then we have to take a sentence which will show all distinctions of all 3 parsers and there dependencies, and how we are dealing with all those and our extension work to all these parsers. This will give us english parsing very close to deep semantics. Which will help us to go in depth of semantics. So our goal is to make a platform for language independent deep semantics (this goal is similar to UD people, but to achieve they are going to shallow level, whereas we are going in depth of semantics.)

How this work is helpful for me?
So this is helpful for overall deep semantics independent of all languages
This is helpful to enhance Anusaaraka MT.
This is also helpful for Alignment tool which is my thesis topic. As parser improves my alignment tool gives me better result.

Deep learning (how to start)
Parsing expert (Needs :  Conscious, continous work. What I will learn :Breadth work, knowing of all parsers, Future Scope:)
Deep Semantics (Needs : Sanskrit knowledge)
CL

Write a proposal
-=============================================

How to run new 2017 stanford parser, its derived scripts etc is there in the following directory:
/home/kishori/anusaaraka/Parsers/stanford-parser/stanford-parser-full-2017-06-09/
You can see multiple outputs of that.

